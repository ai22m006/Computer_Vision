{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification Project 6\n",
    "Choose three classes from the Open Images Dataset. Train a neural net that is able to classify images into these three categories.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "ExecuteTime": {
     "end_time": "2023-06-09T11:21:50.389390700Z",
     "start_time": "2023-06-09T11:21:50.381387700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Cat', 'Dog', 'Person']\n"
     ]
    }
   ],
   "source": [
    "classes = ['Cat', 'Dog', 'Person']\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "https://storage.googleapis.com/openimages/web/visualizer/index.html?type=detection\n",
    "\n",
    "## Base model\n",
    "VGG 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "ExecuteTime": {
     "end_time": "2023-06-09T11:21:53.354220700Z",
     "start_time": "2023-06-09T11:21:53.339700100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(25.663765, shape=(), dtype=float32)\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.applications import VGG19\n",
    "from keras.layers import Dense, Flatten, Conv2D, LeakyReLU\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "import tensorflow as tf\n",
    "import fiftyone as fo\n",
    "import fiftyone.zoo as foz\n",
    "\n",
    "#check if GPU is available\n",
    "print(tf.reduce_sum(tf.random.normal([1000, 1000])))\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task\n",
    "1. Preparation: Split dataset into a 70/30 Train/test split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T11:32:09.724864800Z",
     "start_time": "2023-06-09T11:21:55.652480100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading split 'train' to 'C:\\Users\\Michael\\fiftyone\\open-images-v7\\train' if necessary\n",
      "Downloading 'https://storage.googleapis.com/openimages/2018_04/train/train-images-boxable-with-rotation.csv' to 'C:\\Users\\Michael\\fiftyone\\open-images-v7\\train\\metadata\\image_ids.csv'\n",
      " 100% |██████|    4.8Gb/4.8Gb [1.1m elapsed, 0s remaining, 70.6Mb/s]      \n",
      "Downloading 'https://storage.googleapis.com/openimages/v5/class-descriptions-boxable.csv' to 'C:\\Users\\Michael\\fiftyone\\open-images-v7\\train\\metadata\\classes.csv'\n",
      "Downloading 'https://storage.googleapis.com/openimages/2018_04/bbox_labels_600_hierarchy.json' to 'C:\\Users\\Michael\\AppData\\Local\\Temp\\tmpeyrxl_gq\\metadata\\hierarchy.json'\n",
      "Downloading 'https://storage.googleapis.com/openimages/v5/train-annotations-human-imagelabels-boxable.csv' to 'C:\\Users\\Michael\\fiftyone\\open-images-v7\\train\\labels\\classifications.csv'\n",
      "Downloading 1500 images\n",
      " 100% |█████████████████| 1500/1500 [43.4s elapsed, 0s remaining, 26.5 files/s]      \n",
      "Downloading split 'test' to 'C:\\Users\\Michael\\fiftyone\\open-images-v7\\test' if necessary\n",
      "Downloading 'https://storage.googleapis.com/openimages/2018_04/test/test-images-with-rotation.csv' to 'C:\\Users\\Michael\\fiftyone\\open-images-v7\\test\\metadata\\image_ids.csv'\n",
      "Downloading 'https://storage.googleapis.com/openimages/v5/class-descriptions-boxable.csv' to 'C:\\Users\\Michael\\fiftyone\\open-images-v7\\test\\metadata\\classes.csv'\n",
      "Downloading 'https://storage.googleapis.com/openimages/2018_04/bbox_labels_600_hierarchy.json' to 'C:\\Users\\Michael\\AppData\\Local\\Temp\\tmp5roxwmcl\\metadata\\hierarchy.json'\n",
      "Downloading 'https://storage.googleapis.com/openimages/v5/test-annotations-human-imagelabels-boxable.csv' to 'C:\\Users\\Michael\\fiftyone\\open-images-v7\\test\\labels\\classifications.csv'\n",
      "Only found 1211 (<1500) samples matching your requirements\n",
      "Downloading 1211 images\n",
      " 100% |█████████████████| 1211/1211 [31.4s elapsed, 0s remaining, 36.8 files/s]      \n",
      "Downloading split 'validation' to 'C:\\Users\\Michael\\fiftyone\\open-images-v7\\validation' if necessary\n",
      "Downloading 'https://storage.googleapis.com/openimages/2018_04/validation/validation-images-with-rotation.csv' to 'C:\\Users\\Michael\\fiftyone\\open-images-v7\\validation\\metadata\\image_ids.csv'\n",
      "Downloading 'https://storage.googleapis.com/openimages/v5/class-descriptions-boxable.csv' to 'C:\\Users\\Michael\\fiftyone\\open-images-v7\\validation\\metadata\\classes.csv'\n",
      "Downloading 'https://storage.googleapis.com/openimages/2018_04/bbox_labels_600_hierarchy.json' to 'C:\\Users\\Michael\\AppData\\Local\\Temp\\tmpn2qa7dk5\\metadata\\hierarchy.json'\n",
      "Downloading 'https://storage.googleapis.com/openimages/v5/validation-annotations-human-imagelabels-boxable.csv' to 'C:\\Users\\Michael\\fiftyone\\open-images-v7\\validation\\labels\\classifications.csv'\n",
      "Only found 413 (<1500) samples matching your requirements\n",
      "Downloading 413 images\n",
      " 100% |███████████████████| 413/413 [11.5s elapsed, 0s remaining, 36.2 files/s]      \n",
      "Dataset info written to 'C:\\Users\\Michael\\fiftyone\\open-images-v7\\info.json'\n",
      "Loading 'open-images-v7' split 'train'\n",
      " 100% |███████████████| 1500/1500 [901.1ms elapsed, 0s remaining, 1.7K samples/s]      \n",
      "Loading 'open-images-v7' split 'test'\n",
      " 100% |███████████████| 1211/1211 [638.8ms elapsed, 0s remaining, 1.9K samples/s]      \n",
      "Loading 'open-images-v7' split 'validation'\n",
      " 100% |█████████████████| 413/413 [198.3ms elapsed, 0s remaining, 2.1K samples/s]     \n",
      "Dataset 'cdp-dataset' created\n",
      "Name:        cdp-dataset\n",
      "Media type:  image\n",
      "Num samples: 3124\n",
      "Persistent:  False\n",
      "Tags:        []\n",
      "Sample fields:\n",
      "    id:              fiftyone.core.fields.ObjectIdField\n",
      "    filepath:        fiftyone.core.fields.StringField\n",
      "    tags:            fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n",
      "    metadata:        fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n",
      "    positive_labels: fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classifications)\n",
      "    negative_labels: fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classifications)\n",
      "Downloading split 'train' to 'C:\\Users\\Michael\\fiftyone\\open-images-v7\\train' if necessary\n",
      "Found 21 images, downloading the remaining 1479\n",
      " 100% |█████████████████| 1479/1479 [47.2s elapsed, 0s remaining, 29.8 files/s]      \n",
      "Downloading split 'test' to 'C:\\Users\\Michael\\fiftyone\\open-images-v7\\test' if necessary\n",
      "Found 34 images, downloading the remaining 1466\n",
      " 100% |█████████████████| 1466/1466 [48.7s elapsed, 0s remaining, 15.3 files/s]      \n",
      "Downloading split 'validation' to 'C:\\Users\\Michael\\fiftyone\\open-images-v7\\validation' if necessary\n",
      "Found 36 images, downloading the remaining 1464\n",
      " 100% |█████████████████| 1464/1464 [46.1s elapsed, 0s remaining, 14.6 files/s]      \n",
      "Dataset info written to 'C:\\Users\\Michael\\fiftyone\\open-images-v7\\info.json'\n",
      "Ignoring unsupported parameter 'dataset_path' for importer type <class 'fiftyone.utils.openimages.OpenImagesV7DatasetImporter'>\n",
      "Loading 'open-images-v7' split 'train'\n",
      " 100% |███████████████| 1500/1500 [860.4ms elapsed, 0s remaining, 1.7K samples/s]      \n",
      "Loading 'open-images-v7' split 'test'\n",
      " 100% |███████████████| 1500/1500 [777.1ms elapsed, 0s remaining, 1.9K samples/s]      \n",
      "Loading 'open-images-v7' split 'validation'\n",
      " 100% |███████████████| 1500/1500 [826.3ms elapsed, 0s remaining, 1.8K samples/s]      \n",
      "Dataset 'Dog' created\n",
      "Downloading split 'train' to 'C:\\Users\\Michael\\fiftyone\\open-images-v7\\train' if necessary\n",
      "Found 1 images, downloading the remaining 1499\n",
      " 100% |█████████████████| 1499/1499 [44.6s elapsed, 0s remaining, 38.3 files/s]      \n",
      "Downloading split 'test' to 'C:\\Users\\Michael\\fiftyone\\open-images-v7\\test' if necessary\n",
      "Found 20 images, downloading the remaining 1480\n",
      " 100% |█████████████████| 1480/1480 [45.7s elapsed, 0s remaining, 14.4 files/s]      \n",
      "Downloading split 'validation' to 'C:\\Users\\Michael\\fiftyone\\open-images-v7\\validation' if necessary\n",
      "Found 55 images, downloading the remaining 1445\n",
      " 100% |█████████████████| 1445/1445 [42.5s elapsed, 0s remaining, 24.7 files/s]      \n",
      "Dataset info written to 'C:\\Users\\Michael\\fiftyone\\open-images-v7\\info.json'\n",
      "Ignoring unsupported parameter 'dataset_path' for importer type <class 'fiftyone.utils.openimages.OpenImagesV7DatasetImporter'>\n",
      "Loading 'open-images-v7' split 'train'\n",
      " 100% |███████████████| 1500/1500 [787.6ms elapsed, 0s remaining, 1.9K samples/s]      \n",
      "Loading 'open-images-v7' split 'test'\n",
      " 100% |███████████████| 1500/1500 [778.4ms elapsed, 0s remaining, 1.9K samples/s]      \n",
      "Loading 'open-images-v7' split 'validation'\n",
      " 100% |███████████████| 1500/1500 [770.2ms elapsed, 0s remaining, 2.0K samples/s]      \n",
      "Dataset 'Person' created\n",
      "['Dog', 'Person', 'cdp-dataset']\n"
     ]
    }
   ],
   "source": [
    "ratio_train_test = 0.3\n",
    "num_samples = 1500\n",
    "# Customize where zoo datasets are downloaded\n",
    "dataset_name = \"open-images-v7\"\n",
    "seed = 42\n",
    "splits = ('train', 'test', 'validation')\n",
    "\n",
    "\n",
    "\n",
    "# load cat data\n",
    "chosen_class = classes[0]\n",
    "\n",
    "if fo.dataset_exists(chosen_class):\n",
    "    dataset = fo.load_dataset(chosen_class)\n",
    "\n",
    "dataset = foz.load_zoo_dataset(\n",
    "    name=dataset_name,\n",
    "    splits=splits,\n",
    "    label_types=[\"classifications\"],\n",
    "    classes=chosen_class,\n",
    "    max_samples=num_samples,\n",
    "    seed=seed,\n",
    "    shuffle=True,\n",
    "    dataset_name=\"cdp-dataset\",\n",
    "    only_matching=True,\n",
    ")\n",
    "print(dataset)\n",
    "\n",
    "# load dog data\n",
    "chosen_class = classes[1]\n",
    "\n",
    "if fo.dataset_exists(chosen_class):\n",
    "    dataset = fo.load_dataset(chosen_class)\n",
    "\n",
    "dataset_dog = foz.load_zoo_dataset(\n",
    "    name=dataset_name,\n",
    "    splits=splits,\n",
    "    label_types=[\"classifications\"],\n",
    "    classes=chosen_class,\n",
    "    max_samples=num_samples,\n",
    "    seed=seed,\n",
    "    shuffle=True,\n",
    "    dataset_name=chosen_class,\n",
    "    dataset_path=chosen_class,\n",
    "    only_matching=True,\n",
    ")\n",
    "\n",
    "dataset.merge_samples(dataset_dog)\n",
    "\n",
    "# load person data\n",
    "chosen_class = classes[2]\n",
    "\n",
    "if fo.dataset_exists(chosen_class):\n",
    "    dataset = fo.load_dataset(chosen_class)\n",
    "\n",
    "dataset_person = foz.load_zoo_dataset(\n",
    "    name=dataset_name,\n",
    "    splits=splits,\n",
    "    label_types=[\"classifications\"],\n",
    "    classes=chosen_class,\n",
    "    max_samples=num_samples,\n",
    "    seed=seed,\n",
    "    shuffle=True,\n",
    "    dataset_name=chosen_class,\n",
    "    dataset_path=chosen_class,\n",
    "    only_matching=True,\n",
    ")\n",
    "\n",
    "dataset.merge_samples(dataset_person)\n",
    "\n",
    "print(fo.list_datasets())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['hierarchy', 'classes_map'])\n",
      "['Dog', 'Person', 'cdp-dataset']\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.lib.display.IFrame at 0x1c158a1f1f0>",
      "text/html": "\n        <iframe\n            width=\"100%\"\n            height=\"800\"\n            src=\"http://localhost:5151/?notebook=True&subscription=6f4d8e59-4e88-4e11-9758-a9550fc0c2a9\"\n            frameborder=\"0\"\n            allowfullscreen\n            \n        ></iframe>\n        "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(dataset.info.keys())\n",
    "print(fo.list_datasets())\n",
    "fo.delete_dataset(\"Dog\")\n",
    "fo.delete_dataset(\"Person\")\n",
    "\n",
    "session = fo.launch_app(dataset.view())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-09T11:41:49.963086500Z",
     "start_time": "2023-06-09T11:41:45.781775200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory 'datasets/cdp--tfrecords' already exists; export will be merged with existing files\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No compatible field(s) of type <class 'fiftyone.core.labels.Classification'> found",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[15], line 10\u001B[0m\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m split \u001B[38;5;129;01min\u001B[39;00m splits:\n\u001B[0;32m      9\u001B[0m     split_view \u001B[38;5;241m=\u001B[39m dataset_or_view\u001B[38;5;241m.\u001B[39mmatch_tags(split)\n\u001B[1;32m---> 10\u001B[0m     \u001B[43msplit_view\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexport\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     11\u001B[0m \u001B[43m        \u001B[49m\u001B[43mexport_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mexport_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     12\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdataset_type\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfo\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtypes\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTFImageClassificationDataset\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     13\u001B[0m \u001B[43m        \u001B[49m\u001B[43msplit\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msplit\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     14\u001B[0m \u001B[43m        \u001B[49m\u001B[43mclasses\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mclasses\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     15\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\fiftyone\\core\\collections.py:8076\u001B[0m, in \u001B[0;36mSampleCollection.export\u001B[1;34m(self, export_dir, dataset_type, data_path, labels_path, export_media, rel_dir, dataset_exporter, label_field, frame_labels_field, overwrite, **kwargs)\u001B[0m\n\u001B[0;32m   8073\u001B[0m     export_dir, _ \u001B[38;5;241m=\u001B[39m etau\u001B[38;5;241m.\u001B[39msplit_archive(archive_path)\n\u001B[0;32m   8075\u001B[0m \u001B[38;5;66;03m# Perform the export\u001B[39;00m\n\u001B[1;32m-> 8076\u001B[0m _export(\n\u001B[0;32m   8077\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   8078\u001B[0m     export_dir\u001B[38;5;241m=\u001B[39mexport_dir,\n\u001B[0;32m   8079\u001B[0m     dataset_type\u001B[38;5;241m=\u001B[39mdataset_type,\n\u001B[0;32m   8080\u001B[0m     data_path\u001B[38;5;241m=\u001B[39mdata_path,\n\u001B[0;32m   8081\u001B[0m     labels_path\u001B[38;5;241m=\u001B[39mlabels_path,\n\u001B[0;32m   8082\u001B[0m     export_media\u001B[38;5;241m=\u001B[39mexport_media,\n\u001B[0;32m   8083\u001B[0m     rel_dir\u001B[38;5;241m=\u001B[39mrel_dir,\n\u001B[0;32m   8084\u001B[0m     dataset_exporter\u001B[38;5;241m=\u001B[39mdataset_exporter,\n\u001B[0;32m   8085\u001B[0m     label_field\u001B[38;5;241m=\u001B[39mlabel_field,\n\u001B[0;32m   8086\u001B[0m     frame_labels_field\u001B[38;5;241m=\u001B[39mframe_labels_field,\n\u001B[0;32m   8087\u001B[0m     overwrite\u001B[38;5;241m=\u001B[39moverwrite,\n\u001B[0;32m   8088\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m   8089\u001B[0m )\n\u001B[0;32m   8091\u001B[0m \u001B[38;5;66;03m# Make archive, if requested\u001B[39;00m\n\u001B[0;32m   8092\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m archive_path \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\fiftyone\\core\\collections.py:10547\u001B[0m, in \u001B[0;36m_export\u001B[1;34m(sample_collection, export_dir, dataset_type, data_path, labels_path, export_media, rel_dir, dataset_exporter, label_field, frame_labels_field, overwrite, **kwargs)\u001B[0m\n\u001B[0;32m  10544\u001B[0m \u001B[38;5;66;03m# Get label field(s) to export\u001B[39;00m\n\u001B[0;32m  10545\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(dataset_exporter, foud\u001B[38;5;241m.\u001B[39mLabeledImageDatasetExporter):\n\u001B[0;32m  10546\u001B[0m     \u001B[38;5;66;03m# Labeled images\u001B[39;00m\n\u001B[1;32m> 10547\u001B[0m     label_field \u001B[38;5;241m=\u001B[39m \u001B[43msample_collection\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_parse_label_field\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m  10548\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlabel_field\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m  10549\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdataset_exporter\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdataset_exporter\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m  10550\u001B[0m \u001B[43m        \u001B[49m\u001B[43mallow_coercion\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m  10551\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrequired\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m  10552\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m  10553\u001B[0m     frame_labels_field \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m  10554\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(dataset_exporter, foud\u001B[38;5;241m.\u001B[39mLabeledVideoDatasetExporter):\n\u001B[0;32m  10555\u001B[0m     \u001B[38;5;66;03m# Labeled videos\u001B[39;00m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\fiftyone\\core\\collections.py:9446\u001B[0m, in \u001B[0;36mSampleCollection._parse_label_field\u001B[1;34m(self, label_field, dataset_exporter, allow_coercion, force_dict, required)\u001B[0m\n\u001B[0;32m   9438\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_parse_label_field\u001B[39m(\n\u001B[0;32m   9439\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   9440\u001B[0m     label_field,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   9444\u001B[0m     required\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m   9445\u001B[0m ):\n\u001B[1;32m-> 9446\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_parse_label_field\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   9447\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m   9448\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlabel_field\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   9449\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdataset_exporter\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdataset_exporter\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   9450\u001B[0m \u001B[43m        \u001B[49m\u001B[43mallow_coercion\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mallow_coercion\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   9451\u001B[0m \u001B[43m        \u001B[49m\u001B[43mforce_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mforce_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   9452\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrequired\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequired\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   9453\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\fiftyone\\core\\collections.py:9821\u001B[0m, in \u001B[0;36m_parse_label_field\u001B[1;34m(sample_collection, label_field, dataset_exporter, allow_coercion, force_dict, required)\u001B[0m\n\u001B[0;32m   9818\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m {f: f \u001B[38;5;28;01mfor\u001B[39;00m f \u001B[38;5;129;01min\u001B[39;00m label_field}\n\u001B[0;32m   9820\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m label_field \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m dataset_exporter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m-> 9821\u001B[0m     label_field \u001B[38;5;241m=\u001B[39m \u001B[43m_get_default_label_fields_for_exporter\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   9822\u001B[0m \u001B[43m        \u001B[49m\u001B[43msample_collection\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   9823\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdataset_exporter\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   9824\u001B[0m \u001B[43m        \u001B[49m\u001B[43mallow_coercion\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mallow_coercion\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   9825\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrequired\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequired\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   9826\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   9828\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m label_field \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m required:\n\u001B[0;32m   9829\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m   9830\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnable to find any label fields matching the provided arguments\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   9831\u001B[0m     )\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\fiftyone\\core\\collections.py:9935\u001B[0m, in \u001B[0;36m_get_default_label_fields_for_exporter\u001B[1;34m(sample_collection, dataset_exporter, allow_coercion, required)\u001B[0m\n\u001B[0;32m   9931\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m label_field_or_dict\n\u001B[0;32m   9933\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m required:\n\u001B[0;32m   9934\u001B[0m     \u001B[38;5;66;03m# Strange formatting is because `label_cls` may be a tuple\u001B[39;00m\n\u001B[1;32m-> 9935\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m   9936\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo compatible field(s) of type \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m found\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m (label_cls,)\n\u001B[0;32m   9937\u001B[0m     )\n\u001B[0;32m   9939\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[1;31mValueError\u001B[0m: No compatible field(s) of type <class 'fiftyone.core.labels.Classification'> found"
     ]
    }
   ],
   "source": [
    "export_dir = \"datasets/cdp--tfrecords\"\n",
    "label_field = \"Classifications\"\n",
    "\n",
    "dataset_or_view = fo.load_dataset(\"cdp-dataset\")\n",
    "\n",
    "# Export the dataset as tfrecords\n",
    "\n",
    "tf_records = fo.Dataset.from_dir(\n",
    "    export_dir,\n",
    "    dataset_type=fo.types.TFObjectDetectionDataset,\n",
    "    label_field=label_field,\n",
    "    dataset_name=\"cdp-dataset-\",\n",
    "    only_matching=True,\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-09T11:44:42.013866300Z",
     "start_time": "2023-06-09T11:44:41.913688400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# Define parameters for the loader\n",
    "img_height = 224\n",
    "img_width = 224\n",
    "\n",
    "\n",
    "# Load the training data\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "\n",
    "\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_path,\n",
    "    target_size=(img_height, img_width),\n",
    "    class_mode='categorical',\n",
    "    subset='training')  # set as training data\n",
    "\n",
    "test_generator = train_datagen.flow_from_directory(\n",
    "    test_path,\n",
    "    target_size=(img_height, img_width),\n",
    "    class_mode='categorical',\n",
    "    subset='training')  # set as training data\n",
    "\n",
    "# Load the validation data\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    val_path,\n",
    "    target_size=(img_height, img_width),\n",
    "    class_mode='categorical',\n",
    "    subset='validation')  # set as validation data\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the class labels and the number of images per class from the generators\n",
    "class_labels = train_generator.class_indices\n",
    "n_images_per_class = train_generator.classes.shape[0] // len(class_labels)\n",
    "\n",
    "# Plot the datasets\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "# Plot the training dataset\n",
    "train_generator.class_indices = class_labels\n",
    "class_counts_train = train_generator.classes\n",
    "class_counts_train = [class_counts_train[class_counts_train == i].shape[0] for i in range(len(class_labels))]\n",
    "axes[0].bar(class_labels.keys(), class_counts_train)\n",
    "axes[0].set_title('Training Dataset')\n",
    "\n",
    "# Plot the validation dataset\n",
    "validation_generator.class_indices = class_labels\n",
    "class_counts_val = validation_generator.classes\n",
    "class_counts_val = [class_counts_val[class_counts_val == i].shape[0] for i in range(len(class_labels))]\n",
    "axes[1].bar(class_labels.keys(), class_counts_val)\n",
    "axes[1].set_title('Validation Dataset')\n",
    "\n",
    "# Set the labels and show the plot\n",
    "plt.xlabel('Classes')\n",
    "plt.ylabel('Number of Images')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-08T17:04:53.328600300Z",
     "start_time": "2023-06-08T17:04:53.199967800Z"
    }
   },
   "source": [
    "2. Train a VGG19 network from scratch (randomly initialized weights) and estimate the testset accuracy."
   ],
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 1000x500 with 2 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a VGG19 model with random initialized weights\n",
    "base_model = VGG19(weights=None, include_top=False, input_shape=(img_height, img_width, 3))\n",
    "\n",
    "# Create a new model\n",
    "plain_model = Sequential()\n",
    "# Add the base model as the first layer\n",
    "plain_model.add(base_model)\n",
    "# Flatten the output of the base model\n",
    "plain_model.add(Flatten())\n",
    "# Add the final output layer with softmax activation\n",
    "plain_model.add(Dense(len(classes), activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "plain_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "plain_history = plain_model.fit(\n",
    "    train_generator,\n",
    "    validation_data=validation_generator,\n",
    "    epochs=10)\n",
    "\n",
    "plain_model.save(\"models/plain_model_raw_data.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-08T17:08:52.744716300Z",
     "start_time": "2023-06-08T17:04:55.266999400Z"
    }
   },
   "source": [
    "3. Use an imagenet pretrained VGG19 network, train the model and estimate the testset accuracy."
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "47/99 [=============>................] - ETA: 4:15 - loss: 1.1740 - accuracy: 0.3197"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[16], line 17\u001B[0m\n\u001B[0;32m     14\u001B[0m plain_model\u001B[38;5;241m.\u001B[39mcompile(optimizer\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124madam\u001B[39m\u001B[38;5;124m'\u001B[39m, loss\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcategorical_crossentropy\u001B[39m\u001B[38;5;124m'\u001B[39m, metrics\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124maccuracy\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[0;32m     16\u001B[0m \u001B[38;5;66;03m# Train the model\u001B[39;00m\n\u001B[1;32m---> 17\u001B[0m plain_history \u001B[38;5;241m=\u001B[39m \u001B[43mplain_model\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     18\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrain_generator\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     19\u001B[0m \u001B[43m    \u001B[49m\u001B[43mvalidation_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvalidation_generator\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     20\u001B[0m \u001B[43m    \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     22\u001B[0m plain_model\u001B[38;5;241m.\u001B[39msave(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodels/plain_model_raw_data.h5\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     63\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m     64\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 65\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m     66\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py:1570\u001B[0m, in \u001B[0;36mModel.fit\u001B[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[0;32m   1568\u001B[0m logs \u001B[38;5;241m=\u001B[39m tmp_logs\n\u001B[0;32m   1569\u001B[0m end_step \u001B[38;5;241m=\u001B[39m step \u001B[38;5;241m+\u001B[39m data_handler\u001B[38;5;241m.\u001B[39mstep_increment\n\u001B[1;32m-> 1570\u001B[0m \u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mon_train_batch_end\u001B[49m\u001B[43m(\u001B[49m\u001B[43mend_step\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1571\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstop_training:\n\u001B[0;32m   1572\u001B[0m     \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\keras\\callbacks.py:470\u001B[0m, in \u001B[0;36mCallbackList.on_train_batch_end\u001B[1;34m(self, batch, logs)\u001B[0m\n\u001B[0;32m    463\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Calls the `on_train_batch_end` methods of its callbacks.\u001B[39;00m\n\u001B[0;32m    464\u001B[0m \n\u001B[0;32m    465\u001B[0m \u001B[38;5;124;03mArgs:\u001B[39;00m\n\u001B[0;32m    466\u001B[0m \u001B[38;5;124;03m    batch: Integer, index of batch within the current epoch.\u001B[39;00m\n\u001B[0;32m    467\u001B[0m \u001B[38;5;124;03m    logs: Dict. Aggregated metric results up until this batch.\u001B[39;00m\n\u001B[0;32m    468\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    469\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_should_call_train_batch_hooks:\n\u001B[1;32m--> 470\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_batch_hook\u001B[49m\u001B[43m(\u001B[49m\u001B[43mModeKeys\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTRAIN\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mend\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlogs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\keras\\callbacks.py:317\u001B[0m, in \u001B[0;36mCallbackList._call_batch_hook\u001B[1;34m(self, mode, hook, batch, logs)\u001B[0m\n\u001B[0;32m    315\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_batch_begin_hook(mode, batch, logs)\n\u001B[0;32m    316\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m hook \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mend\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m--> 317\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_batch_end_hook\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    318\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    319\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    320\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnrecognized hook: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mhook\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    321\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mExpected values are [\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbegin\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mend\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m]\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m    322\u001B[0m     )\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\keras\\callbacks.py:340\u001B[0m, in \u001B[0;36mCallbackList._call_batch_end_hook\u001B[1;34m(self, mode, batch, logs)\u001B[0m\n\u001B[0;32m    337\u001B[0m     batch_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime() \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_batch_start_time\n\u001B[0;32m    338\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_batch_times\u001B[38;5;241m.\u001B[39mappend(batch_time)\n\u001B[1;32m--> 340\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_batch_hook_helper\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhook_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    342\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_batch_times) \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_batches_for_timing_check:\n\u001B[0;32m    343\u001B[0m     end_hook_name \u001B[38;5;241m=\u001B[39m hook_name\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\keras\\callbacks.py:388\u001B[0m, in \u001B[0;36mCallbackList._call_batch_hook_helper\u001B[1;34m(self, hook_name, batch, logs)\u001B[0m\n\u001B[0;32m    386\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m callback \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcallbacks:\n\u001B[0;32m    387\u001B[0m     hook \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(callback, hook_name)\n\u001B[1;32m--> 388\u001B[0m     \u001B[43mhook\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    390\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_timing:\n\u001B[0;32m    391\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m hook_name \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_hook_times:\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\keras\\callbacks.py:1081\u001B[0m, in \u001B[0;36mProgbarLogger.on_train_batch_end\u001B[1;34m(self, batch, logs)\u001B[0m\n\u001B[0;32m   1080\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mon_train_batch_end\u001B[39m(\u001B[38;5;28mself\u001B[39m, batch, logs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m-> 1081\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_batch_update_progbar\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\keras\\callbacks.py:1157\u001B[0m, in \u001B[0;36mProgbarLogger._batch_update_progbar\u001B[1;34m(self, batch, logs)\u001B[0m\n\u001B[0;32m   1153\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mseen \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m add_seen\n\u001B[0;32m   1155\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mverbose \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m   1156\u001B[0m     \u001B[38;5;66;03m# Only block async when verbose = 1.\u001B[39;00m\n\u001B[1;32m-> 1157\u001B[0m     logs \u001B[38;5;241m=\u001B[39m \u001B[43mtf_utils\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msync_to_numpy_or_python_type\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlogs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1158\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprogbar\u001B[38;5;241m.\u001B[39mupdate(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mseen, \u001B[38;5;28mlist\u001B[39m(logs\u001B[38;5;241m.\u001B[39mitems()), finalize\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\tf_utils.py:635\u001B[0m, in \u001B[0;36msync_to_numpy_or_python_type\u001B[1;34m(tensors)\u001B[0m\n\u001B[0;32m    632\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m t\n\u001B[0;32m    633\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m t\u001B[38;5;241m.\u001B[39mitem() \u001B[38;5;28;01mif\u001B[39;00m np\u001B[38;5;241m.\u001B[39mndim(t) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m t\n\u001B[1;32m--> 635\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap_structure\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_to_single_numpy_or_python_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:917\u001B[0m, in \u001B[0;36mmap_structure\u001B[1;34m(func, *structure, **kwargs)\u001B[0m\n\u001B[0;32m    913\u001B[0m flat_structure \u001B[38;5;241m=\u001B[39m (flatten(s, expand_composites) \u001B[38;5;28;01mfor\u001B[39;00m s \u001B[38;5;129;01min\u001B[39;00m structure)\n\u001B[0;32m    914\u001B[0m entries \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mzip\u001B[39m(\u001B[38;5;241m*\u001B[39mflat_structure)\n\u001B[0;32m    916\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m pack_sequence_as(\n\u001B[1;32m--> 917\u001B[0m     structure[\u001B[38;5;241m0\u001B[39m], [func(\u001B[38;5;241m*\u001B[39mx) \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m entries],\n\u001B[0;32m    918\u001B[0m     expand_composites\u001B[38;5;241m=\u001B[39mexpand_composites)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:917\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    913\u001B[0m flat_structure \u001B[38;5;241m=\u001B[39m (flatten(s, expand_composites) \u001B[38;5;28;01mfor\u001B[39;00m s \u001B[38;5;129;01min\u001B[39;00m structure)\n\u001B[0;32m    914\u001B[0m entries \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mzip\u001B[39m(\u001B[38;5;241m*\u001B[39mflat_structure)\n\u001B[0;32m    916\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m pack_sequence_as(\n\u001B[1;32m--> 917\u001B[0m     structure[\u001B[38;5;241m0\u001B[39m], [\u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m entries],\n\u001B[0;32m    918\u001B[0m     expand_composites\u001B[38;5;241m=\u001B[39mexpand_composites)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\tf_utils.py:628\u001B[0m, in \u001B[0;36msync_to_numpy_or_python_type.<locals>._to_single_numpy_or_python_type\u001B[1;34m(t)\u001B[0m\n\u001B[0;32m    625\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_to_single_numpy_or_python_type\u001B[39m(t):\n\u001B[0;32m    626\u001B[0m     \u001B[38;5;66;03m# Don't turn ragged or sparse tensors to NumPy.\u001B[39;00m\n\u001B[0;32m    627\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(t, tf\u001B[38;5;241m.\u001B[39mTensor):\n\u001B[1;32m--> 628\u001B[0m         t \u001B[38;5;241m=\u001B[39m \u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnumpy\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    629\u001B[0m     \u001B[38;5;66;03m# Strings, ragged and sparse tensors don't have .item(). Return them\u001B[39;00m\n\u001B[0;32m    630\u001B[0m     \u001B[38;5;66;03m# as-is.\u001B[39;00m\n\u001B[0;32m    631\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(t, (np\u001B[38;5;241m.\u001B[39mndarray, np\u001B[38;5;241m.\u001B[39mgeneric)):\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1157\u001B[0m, in \u001B[0;36m_EagerTensorBase.numpy\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1134\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Copy of the contents of this Tensor into a NumPy array or scalar.\u001B[39;00m\n\u001B[0;32m   1135\u001B[0m \n\u001B[0;32m   1136\u001B[0m \u001B[38;5;124;03mUnlike NumPy arrays, Tensors are immutable, so this method has to copy\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1154\u001B[0m \u001B[38;5;124;03m    NumPy dtype.\u001B[39;00m\n\u001B[0;32m   1155\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1156\u001B[0m \u001B[38;5;66;03m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001B[39;00m\n\u001B[1;32m-> 1157\u001B[0m maybe_arr \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_numpy\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# pylint: disable=protected-access\u001B[39;00m\n\u001B[0;32m   1158\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m maybe_arr\u001B[38;5;241m.\u001B[39mcopy() \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(maybe_arr, np\u001B[38;5;241m.\u001B[39mndarray) \u001B[38;5;28;01melse\u001B[39;00m maybe_arr\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1123\u001B[0m, in \u001B[0;36m_EagerTensorBase._numpy\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1121\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_numpy\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m   1122\u001B[0m   \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1123\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_numpy_internal\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1124\u001B[0m   \u001B[38;5;28;01mexcept\u001B[39;00m core\u001B[38;5;241m.\u001B[39m_NotOkStatusException \u001B[38;5;28;01mas\u001B[39;00m e:  \u001B[38;5;66;03m# pylint: disable=protected-access\u001B[39;00m\n\u001B[0;32m   1125\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m core\u001B[38;5;241m.\u001B[39m_status_to_exception(e) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a VGG19 model with random initialized weights\n",
    "pre_trained_model = VGG19(weights='imagenet', include_top=False, input_shape=(img_height, img_width, 3))\n",
    "\n",
    "# Create a new model\n",
    "trained_model = Sequential()\n",
    "# Add the base model as the first layer\n",
    "trained_model.add(pre_trained_model)\n",
    "# Flatten the output of the base model\n",
    "trained_model.add(Flatten())\n",
    "# Add the final output layer with softmax activation\n",
    "trained_model.add(Dense(len(classes), activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "trained_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "trained_history  = trained_model.fit(\n",
    "    train_generator,\n",
    "    validation_data=validation_generator,\n",
    "    epochs=10)\n",
    "\n",
    "trained_model.save(\"models/trained_model_raw_data.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The differences in loss and accuracy of the plain and pre trained network over the first 10 epochs"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract loss and accuracy values for both models\n",
    "plain_loss = plain_history.history['loss']\n",
    "plain_accuracy = plain_history.history['accuracy']\n",
    "trained_loss = trained_history.history['loss']\n",
    "trained_accuracy = trained_history.history['accuracy']\n",
    "\n",
    "# Plot the loss curves\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(1, 11), plain_loss, 'b-', label='Plain Model')\n",
    "plt.plot(range(1, 11), trained_loss, 'r-', label='Pre-trained Model')\n",
    "plt.title('Loss Curves')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot the accuracy curves\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(1, 11), plain_accuracy, 'b-', label='Plain Model')\n",
    "plt.plot(range(1, 11), trained_accuracy, 'r-', label='Pre-trained Model')\n",
    "plt.title('Accuracy Curves')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Data cleansing: Remove “bad” images from the dataset. Which did you remove? How many? Discuss results."
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Add data augmentation: \n",
    "    - Random flip\n",
    "    - Random contrast\n",
    "    - Random translation"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_preprocessing(image):\n",
    "    # Generate a random contrast factor\n",
    "    contrast_factor = np.random.uniform(0.8, 1.2)\n",
    "    # Apply contrast adjustment\n",
    "    image = image * contrast_factor\n",
    "    image = tf.clip_by_value(image, 0.0, 1.0)  # Clip values to the [0, 1] range\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters for the loader\n",
    "batch_size = 10\n",
    "img_height = 224\n",
    "img_width = 224\n",
    "\n",
    "# Load the training data\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1. / 255,\n",
    "        validation_split=0.3,\n",
    "        horizontal_flip=True,  # Apply random flip\n",
    "        vertical_flip=True,  # Apply random flip\n",
    "        width_shift_range=0.2,  # Apply random translation\n",
    "        height_shift_range=0.2,  # Apply random translation\n",
    "        preprocessing_function=custom_preprocessing # Apply random contrast\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    base_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='training')  # set as training data\n",
    "\n",
    "# Load the validation data\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    base_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='validation')  # set as validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train both models again"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a VGG19 model with random initialized weights\n",
    "base_model = VGG19(weights=None, include_top=False, input_shape=(img_height, img_width, 3))\n",
    "\n",
    "# Create a new model\n",
    "plain_model = Sequential()\n",
    "# Add the base model as the first layer\n",
    "plain_model.add(base_model)\n",
    "# Flatten the output of the base model\n",
    "plain_model.add(Flatten())\n",
    "# Add the final output layer with softmax activation\n",
    "plain_model.add(Dense(len(classes), activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "plain_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "plain_history = plain_model.fit(\n",
    "    train_generator,\n",
    "    validation_data=validation_generator,\n",
    "    epochs=10)\n",
    "\n",
    "plain_model.save(\"models/plain_model_augmentated_data.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a VGG19 model with random initialized weights\n",
    "pre_trained_model = VGG19(weights='imagenet', include_top=False, input_shape=(img_height, img_width, 3))\n",
    "\n",
    "# Create a new model\n",
    "trained_model = Sequential()\n",
    "# Add the base model as the first layer\n",
    "trained_model.add(pre_trained_model)\n",
    "# Flatten the output of the base model\n",
    "trained_model.add(Flatten())\n",
    "# Add the final output layer with softmax activation\n",
    "trained_model.add(Dense(len(classes), activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "trained_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "trained_history  = trained_model.fit(\n",
    "    train_generator,\n",
    "    validation_data=validation_generator,\n",
    "    epochs=10)\n",
    "\n",
    "trained_model.save(\"models/trained_model_augmentated_data.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract loss and accuracy values for both models\n",
    "plain_loss = plain_history.history['loss']\n",
    "plain_accuracy = plain_history.history['accuracy']\n",
    "trained_loss = trained_history.history['loss']\n",
    "trained_accuracy = trained_history.history['accuracy']\n",
    "\n",
    "# Plot the loss curves\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(1, 11), plain_loss, 'b-', label='Plain Model')\n",
    "plt.plot(range(1, 11), trained_loss, 'r-', label='Pre-trained Model')\n",
    "plt.title('Loss Curves')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot the accuracy curves\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(1, 11), plain_accuracy, 'b-', label='Plain Model')\n",
    "plt.plot(range(1, 11), trained_accuracy, 'r-', label='Pre-trained Model')\n",
    "plt.title('Accuracy Curves')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Rebuild VGG19. After layer block4_conv4 (25, 25, 512):\n",
    "    - Add inception layer with dimensionality reduction (no of output filters should be 512, choose own values for the filter dimensionality reduction in 1x1 layers)\n",
    "    - Add conv layer (kernel 1x1,  filters 1024, padding valid, stride 1, activation leaky relu)\n",
    "    - Add conv layer (kernel 3x3,  filters 1024, padding same, stride 1, activation relu)\n",
    "    - Freeze conv2 layers and before"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a VGG19 model with random initialized weights\n",
    "base_model = VGG19(weights=None, include_top=False, input_shape=(img_height, img_width, 3))\n",
    "\n",
    "# Create a new model\n",
    "plain_model = Sequential()\n",
    "\n",
    "# Add layers until block4_conv4\n",
    "for layer in base_model.layers:\n",
    "    plain_model.add(layer)\n",
    "    if layer.name == 'block4_conv4':\n",
    "        break\n",
    "\n",
    "# Freeze the layers before block2_conv2\n",
    "freeze = True\n",
    "for layer in plain_model.layers:\n",
    "    if layer.name == 'block2_conv2':\n",
    "        freeze = False\n",
    "    layer.trainable = not freeze\n",
    "\n",
    "# Add the inception layer with dimensionality reduction\n",
    "plain_model.add(Conv2D(512, (1, 1), activation='relu'))\n",
    "plain_model.add(Conv2D(256, (1, 1), activation='relu'))\n",
    "plain_model.add(Conv2D(512, (3, 3), padding='same', activation='relu'))\n",
    "\n",
    "# Add the conv layer with kernel 1x1, filters 1024, padding valid, stride 1, activation leaky relu\n",
    "plain_model.add(Conv2D(1024, (1, 1), padding='valid', strides=1, activation=LeakyReLU(alpha=0.1)))\n",
    "\n",
    "# Add the conv layer with kernel 3x3, filters 1024, padding same, stride 1, activation relu\n",
    "plain_model.add(Conv2D(1024, (3, 3), padding='same', strides=1, activation='relu'))\n",
    "\n",
    "# Flatten the output of the last added layer\n",
    "plain_model.add(Flatten())\n",
    "\n",
    "# Add the final output layer with softmax activation\n",
    "plain_model.add(Dense(len(classes), activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "plain_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "plain_history = plain_model.fit(\n",
    "    train_generator,\n",
    "    validation_data=validation_generator,\n",
    "    epochs=10)\n",
    "\n",
    "plain_model.save(\"models/plain_Rebuild_VGG19_model_augmentated_data.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a VGG19 model with random initialized weights\n",
    "pre_trained_model = VGG19(weights='imagenet', include_top=False, input_shape=(img_height, img_width, 3))\n",
    "\n",
    "# Create a new model\n",
    "trained_model = Sequential()\n",
    "\n",
    "# Add layers until block4_conv4\n",
    "for layer in pre_trained_model.layers:\n",
    "    trained_model.add(layer)\n",
    "    if layer.name == 'block4_conv4':\n",
    "        break\n",
    "\n",
    "# Freeze the layers before block2_conv2\n",
    "freeze = True\n",
    "for layer in trained_model.layers:\n",
    "    if layer.name == 'block2_conv2':\n",
    "        freeze = False\n",
    "    layer.trainable = not freeze\n",
    "\n",
    "# Add the inception layer with dimensionality reduction\n",
    "trained_model.add(Conv2D(512, (1, 1), activation='relu'))\n",
    "trained_model.add(Conv2D(256, (1, 1), activation='relu'))\n",
    "trained_model.add(Conv2D(512, (3, 3), padding='same', activation='relu'))\n",
    "\n",
    "# Add the conv layer with kernel 1x1, filters 1024, padding valid, stride 1, activation leaky relu\n",
    "trained_model.add(Conv2D(1024, (1, 1), padding='valid', strides=1, activation=LeakyReLU(alpha=0.1)))\n",
    "\n",
    "# Add the conv layer with kernel 3x3, filters 1024, padding same, stride 1, activation relu\n",
    "trained_model.add(Conv2D(1024, (3, 3), padding='same', strides=1, activation='relu'))\n",
    "\n",
    "# Flatten the output of the last added layer\n",
    "trained_model.add(Flatten())\n",
    "\n",
    "# Add the final output layer with softmax activation\n",
    "trained_model.add(Dense(len(classes), activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "trained_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "trained_history  = trained_model.fit(\n",
    "    train_generator,\n",
    "    validation_data=validation_generator,\n",
    "    epochs=10)\n",
    "\n",
    "trained_model.save(\"models/trained_Rebuild_VGG19_model_augmentated_data.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract loss and accuracy values for both models\n",
    "plain_loss = plain_history.history['loss']\n",
    "plain_accuracy = plain_history.history['accuracy']\n",
    "trained_loss = trained_history.history['loss']\n",
    "trained_accuracy = trained_history.history['accuracy']\n",
    "\n",
    "# Plot the loss curves\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(1, 11), plain_loss, 'b-', label='Plain Model')\n",
    "plt.plot(range(1, 11), trained_loss, 'r-', label='Pre-trained Model')\n",
    "plt.title('Loss Curves')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot the accuracy curves\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(1, 11), plain_accuracy, 'b-', label='Plain Model')\n",
    "plt.plot(range(1, 11), trained_accuracy, 'r-', label='Pre-trained Model')\n",
    "plt.title('Accuracy Curves')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Test a few of your own images and present the results"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Answer the following questions:\n",
    "    - What accuracy can be achieved? What is the accuracy of the train vs. test set?\n",
    "    - On what infrastructure did you train it? What is the inference time?\n",
    "    - What are the number of parameters of the model?\n",
    "    - Which categories are most likely to be confused by the algorithm? Show results in a confusion matrix."
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
