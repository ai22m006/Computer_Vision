{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# import lib\n",
    "import fiftyone.zoo as foz\n",
    "import fiftyone as fo\n",
    "from fiftyone import ViewField as F\n",
    "from keras.applications import VGG19\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define samples count\n",
    "subset_samples = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading split 'validation' to 'C:\\Users\\npaul\\fiftyone\\open-images-v7\\validation' if necessary\n",
      "Necessary images already downloaded\n",
      "Existing download of split 'validation' is sufficient\n",
      "Loading 'open-images-v7' split 'validation'\n",
      " 100% |█████████████████| 250/250 [2.2s elapsed, 0s remaining, 120.5 samples/s]      \n",
      "Dataset 'person_subset' created\n",
      "Downloading split 'validation' to 'C:\\Users\\npaul\\fiftyone\\open-images-v7\\validation' if necessary\n",
      "Necessary images already downloaded\n",
      "Existing download of split 'validation' is sufficient\n",
      "Loading 'open-images-v7' split 'validation'\n",
      " 100% |█████████████████| 250/250 [2.5s elapsed, 0s remaining, 95.3 samples/s]       \n",
      "Dataset 'cat_subset' created\n",
      "Downloading split 'validation' to 'C:\\Users\\npaul\\fiftyone\\open-images-v7\\validation' if necessary\n",
      "Necessary images already downloaded\n",
      "Existing download of split 'validation' is sufficient\n",
      "Loading 'open-images-v7' split 'validation'\n",
      " 100% |█████████████████| 250/250 [1.7s elapsed, 0s remaining, 149.4 samples/s]         \n",
      "Dataset 'dog_subset' created\n"
     ]
    }
   ],
   "source": [
    "# List all available datasets\n",
    "datasets = fo.list_datasets()\n",
    "# Delete 'person_subset' dataset if it exists\n",
    "if 'person_subset' in datasets:\n",
    "    fo.delete_dataset('person_subset')\n",
    "# Delete 'dog_subset' dataset if it exists\n",
    "if 'dog_subset' in datasets:\n",
    "    fo.delete_dataset('dog_subset')\n",
    "# Delete 'cat_subset' dataset if it exists\n",
    "if 'cat_subset' in datasets:\n",
    "    fo.delete_dataset('cat_subset')\n",
    "\n",
    "# Load a split of a zoo dataset\n",
    "person_subset = foz.load_zoo_dataset(\n",
    "    \"open-images-v7\",\n",
    "    split=\"validation\",\n",
    "    label_types=[\"classifications\"],\n",
    "    classes=[\"Person\"],\n",
    "    max_samples=subset_samples,\n",
    "    seed=51,\n",
    "    shuffle=True,\n",
    "    dataset_name=\"person_subset\",\n",
    ")\n",
    "\n",
    "cat_subset = foz.load_zoo_dataset(\n",
    "    \"open-images-v7\",\n",
    "    split=\"validation\",\n",
    "    label_types=[\"classifications\"],\n",
    "    classes=[\"Cat\"],\n",
    "    max_samples=subset_samples,\n",
    "    seed=51,\n",
    "    shuffle=True,\n",
    "    dataset_name=\"cat_subset\",\n",
    ")\n",
    "\n",
    "dog_subset = foz.load_zoo_dataset(\n",
    "    \"open-images-v7\",\n",
    "    split=\"validation\",\n",
    "    label_types=[\"classifications\"],\n",
    "    classes=[\"Dog\"],\n",
    "    max_samples=subset_samples,\n",
    "    seed=51,\n",
    "    shuffle=True,\n",
    "    dataset_name=\"dog_subset\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cat_subset', 'dog_subset', 'person_subset']\n"
     ]
    }
   ],
   "source": [
    "# print out list of available datasets\n",
    "print(fo.list_datasets())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset:     person_subset\n",
      "Media type:  image\n",
      "Num samples: 675\n",
      "Sample fields:\n",
      "    id:              fiftyone.core.fields.ObjectIdField\n",
      "    filepath:        fiftyone.core.fields.StringField\n",
      "    tags:            fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n",
      "    metadata:        fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n",
      "    positive_labels: fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classifications)\n",
      "    negative_labels: fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classifications)\n",
      "View stages:\n",
      "    ---\n",
      "Computing metadata...\n",
      " 100% |█████████████████| 675/675 [17.0s elapsed, 0s remaining, 163.6 samples/s]      \n",
      "{\n",
      "    'samples_count': 675,\n",
      "    'samples_bytes': 764616,\n",
      "    'samples_size': '746.7KB',\n",
      "    'media_bytes': 201852636,\n",
      "    'media_size': '192.5MB',\n",
      "    'total_bytes': 202617252,\n",
      "    'total_size': '193.2MB',\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "_dataset = fo.load_dataset(\"person_subset\")\n",
    "dog_subset = fo.load_dataset(\"dog_subset\")\n",
    "cat_subset = fo.load_dataset(\"cat_subset\")\n",
    "# Merge the samples together into the same dataset\n",
    "_dataset.merge_samples(dog_subset)\n",
    "_dataset.merge_samples(cat_subset)\n",
    "print(_dataset.view())\n",
    "fo.pprint(_dataset.stats(include_media=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly shuffle the order of the samples in the dataset\n",
    "_dataset = _dataset.shuffle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 472\n",
      "Number of testing samples: 203\n"
     ]
    }
   ],
   "source": [
    "sample_ids = _dataset.values(\"id\")\n",
    "# Split the sample IDs\n",
    "train_ids, val_ids = train_test_split(sample_ids, test_size=0.3, random_state=42)\n",
    "# Get the corresponding samples for training and testing\n",
    "train_dataset = _dataset.select(train_ids)\n",
    "val_dataset = _dataset.select(val_ids)\n",
    "# Print the number of samples in each split\n",
    "print(\"Number of training samples:\", len(train_dataset))\n",
    "print(\"Number of testing samples:\", len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model parametes\n",
    "num_classes = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model_scratch = Sequential()\n",
    "model_scratch.add(VGG19(include_top=False, weights=None, input_shape=(224, 224, 3)))  # VGG19 without the top layer\n",
    "model_scratch.add(Flatten())\n",
    "model_scratch.add(Dense(256, activation='relu'))\n",
    "model_scratch.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[177], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m model_scratch\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mAdam(), loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m model_scratch\u001b[38;5;241m.\u001b[39mfit(\u001b[43mX_train\u001b[49m, y_train, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, validation_data\u001b[38;5;241m=\u001b[39m(X_test, y_test))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "model_scratch.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# Train the model\n",
    "model_scratch.fit(X_train, y_train, batch_size=32, epochs=10, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
