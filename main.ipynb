{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification Project 6\n",
    "Choose three classes from the Open Images Dataset. Train a neural net that is able to classify images into these three categories.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "ExecuteTime": {
     "end_time": "2023-06-06T12:34:06.101264300Z",
     "start_time": "2023-06-06T12:34:06.052309300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Cat', 'Dog', 'Person']\n"
     ]
    }
   ],
   "source": [
    "classes = ['Cat', 'Dog', 'Person']\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "https://storage.googleapis.com/openimages/web/visualizer/index.html?type=detection\n",
    "\n",
    "## Base model\n",
    "VGG 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "ExecuteTime": {
     "end_time": "2023-06-06T12:34:06.102261600Z",
     "start_time": "2023-06-06T12:34:06.059137900Z"
    }
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "from openimages import download\n",
    "from keras.applications import VGG19\n",
    "from keras.layers import Dense, Flatten, Conv2D, LeakyReLU\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "import tensorflow as tf\n",
    "import keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-06T12:39:26.349255800Z",
     "start_time": "2023-06-06T12:34:06.062725200Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-06  14:35:56 INFO Downloading 1500 train images for class 'cat'\n",
      "100%|██████████| 1500/1500 [00:59<00:00, 25.14it/s]\n",
      "2023-06-06  14:36:56 INFO Downloading 1500 train images for class 'dog'\n",
      "100%|█████████▉| 1494/1500 [01:21<00:00, 16.97it/s]2023-06-06  14:38:17 WARNING Connection pool is full, discarding connection: open-images-dataset.s3.amazonaws.com. Connection pool size: 10\n",
      "100%|██████████| 1500/1500 [01:21<00:00, 18.42it/s]\n",
      "2023-06-06  14:38:18 INFO Downloading 1500 train images for class 'person'\n",
      "100%|██████████| 1500/1500 [01:00<00:00, 24.93it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'cat': {'images_dir': './dataset\\\\cat\\\\images'},\n 'dog': {'images_dir': './dataset\\\\dog\\\\images'},\n 'person': {'images_dir': './dataset\\\\person\\\\images'}}"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Path to the directory where the images are stored\n",
    "base_dir = './dataset'\n",
    "\n",
    "# Check if the directory exists\n",
    "if os.path.exists(base_dir):\n",
    "    # Delete the directory and its contents\n",
    "    shutil.rmtree(base_dir)\n",
    "\n",
    "# number of images per class\n",
    "n_images = 1500\n",
    "\n",
    "# Download images for each class using Open Images\n",
    "download.download_dataset(\n",
    "    dest_dir=base_dir,\n",
    "    class_labels=classes,\n",
    "    limit=n_images\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task\n",
    "1. Preparation: Split dataset into a 70/30 Train/test split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-06T12:39:26.436925400Z",
     "start_time": "2023-06-06T12:39:26.351258400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3150 images belonging to 3 classes.\n",
      "Found 1350 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "# Define parameters for the loader\n",
    "img_height = 224\n",
    "img_width = 224\n",
    "\n",
    "# Load the training data\n",
    "train_datagen = ImageDataGenerator(rescale=1. / 255, validation_split=0.3)  # set validation split\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    base_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    class_mode='categorical',\n",
    "    subset='training')  # set as training data\n",
    "\n",
    "# Load the validation data\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    base_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    class_mode='categorical',\n",
    "    subset='validation')  # set as validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-06T12:39:26.526220100Z",
     "start_time": "2023-06-06T12:39:26.424418500Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 1000x500 with 2 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the class labels and the number of images per class from the generators\n",
    "class_labels = train_generator.class_indices\n",
    "n_images_per_class = train_generator.classes.shape[0] // len(class_labels)\n",
    "\n",
    "# Plot the datasets\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "# Plot the training dataset\n",
    "train_generator.class_indices = class_labels\n",
    "class_counts_train = train_generator.classes\n",
    "class_counts_train = [class_counts_train[class_counts_train == i].shape[0] for i in range(len(class_labels))]\n",
    "axes[0].bar(class_labels.keys(), class_counts_train)\n",
    "axes[0].set_title('Training Dataset')\n",
    "\n",
    "# Plot the validation dataset\n",
    "validation_generator.class_indices = class_labels\n",
    "class_counts_val = validation_generator.classes\n",
    "class_counts_val = [class_counts_val[class_counts_val == i].shape[0] for i in range(len(class_labels))]\n",
    "axes[1].bar(class_labels.keys(), class_counts_val)\n",
    "axes[1].set_title('Validation Dataset')\n",
    "\n",
    "# Set the labels and show the plot\n",
    "plt.xlabel('Classes')\n",
    "plt.ylabel('Number of Images')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Train a VGG19 network from scratch (randomly initialized weights) and estimate the testset accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-06T14:03:40.732193300Z",
     "start_time": "2023-06-06T12:39:26.527220Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "99/99 [==============================] - 508s 5s/step - loss: 1.2068 - accuracy: 0.3171 - val_loss: 1.0987 - val_accuracy: 0.3333\n",
      "Epoch 2/10\n",
      "99/99 [==============================] - 507s 5s/step - loss: 1.0988 - accuracy: 0.3270 - val_loss: 1.0986 - val_accuracy: 0.3333\n",
      "Epoch 3/10\n",
      "99/99 [==============================] - 504s 5s/step - loss: 1.0987 - accuracy: 0.3181 - val_loss: 1.0986 - val_accuracy: 0.3333\n",
      "Epoch 4/10\n",
      "99/99 [==============================] - 505s 5s/step - loss: 1.0987 - accuracy: 0.3308 - val_loss: 1.0986 - val_accuracy: 0.3333\n",
      "Epoch 5/10\n",
      "99/99 [==============================] - 511s 5s/step - loss: 1.0987 - accuracy: 0.3273 - val_loss: 1.0986 - val_accuracy: 0.3333\n",
      "Epoch 6/10\n",
      "99/99 [==============================] - 509s 5s/step - loss: 1.0987 - accuracy: 0.3244 - val_loss: 1.0986 - val_accuracy: 0.3333\n",
      "Epoch 7/10\n",
      "99/99 [==============================] - 508s 5s/step - loss: 1.0987 - accuracy: 0.3171 - val_loss: 1.0986 - val_accuracy: 0.3333\n",
      "Epoch 8/10\n",
      "99/99 [==============================] - 499s 5s/step - loss: 1.0987 - accuracy: 0.3203 - val_loss: 1.0986 - val_accuracy: 0.3333\n",
      "Epoch 9/10\n",
      "99/99 [==============================] - 501s 5s/step - loss: 1.0987 - accuracy: 0.3260 - val_loss: 1.0986 - val_accuracy: 0.3333\n",
      "Epoch 10/10\n",
      "99/99 [==============================] - 501s 5s/step - loss: 1.0987 - accuracy: 0.3295 - val_loss: 1.0986 - val_accuracy: 0.3333\n"
     ]
    }
   ],
   "source": [
    "# Load a VGG19 model with random initialized weights\n",
    "base_model = VGG19(weights=None, include_top=False, input_shape=(img_height, img_width, 3))\n",
    "\n",
    "# Create a new model\n",
    "plain_model = Sequential()\n",
    "# Add the base model as the first layer\n",
    "plain_model.add(base_model)\n",
    "# Flatten the output of the base model\n",
    "plain_model.add(Flatten())\n",
    "# Add the final output layer with softmax activation\n",
    "plain_model.add(Dense(len(classes), activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "plain_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "plain_history = plain_model.fit(\n",
    "    train_generator,\n",
    "    validation_data=validation_generator,\n",
    "    epochs=10)\n",
    "\n",
    "plain_model.save(\"models/plain_model_raw_data.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Use an imagenet pretrained VGG19 network, train the model and estimate the testset accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-06T14:08:19.287645800Z",
     "start_time": "2023-06-06T14:03:40.740801800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "59/99 [================>.............] - ETA: 3:04 - loss: 2.3131 - accuracy: 0.3283"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[14], line 17\u001B[0m\n\u001B[0;32m     14\u001B[0m trained_model\u001B[38;5;241m.\u001B[39mcompile(optimizer\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124madam\u001B[39m\u001B[38;5;124m'\u001B[39m, loss\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcategorical_crossentropy\u001B[39m\u001B[38;5;124m'\u001B[39m, metrics\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124maccuracy\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[0;32m     16\u001B[0m \u001B[38;5;66;03m# Train the model\u001B[39;00m\n\u001B[1;32m---> 17\u001B[0m trained_history  \u001B[38;5;241m=\u001B[39m \u001B[43mtrained_model\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     18\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrain_generator\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     19\u001B[0m \u001B[43m    \u001B[49m\u001B[43mvalidation_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvalidation_generator\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     20\u001B[0m \u001B[43m    \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     22\u001B[0m trained_model\u001B[38;5;241m.\u001B[39msave(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrained_model_raw_data.h5\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32mC:\\dev\\repos\\FH\\Computer_Vision\\venv\\Lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     63\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m     64\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 65\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     66\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[1;32mC:\\dev\\repos\\FH\\Computer_Vision\\venv\\Lib\\site-packages\\keras\\engine\\training.py:1685\u001B[0m, in \u001B[0;36mModel.fit\u001B[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[0;32m   1677\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m tf\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mexperimental\u001B[38;5;241m.\u001B[39mTrace(\n\u001B[0;32m   1678\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   1679\u001B[0m     epoch_num\u001B[38;5;241m=\u001B[39mepoch,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1682\u001B[0m     _r\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m,\n\u001B[0;32m   1683\u001B[0m ):\n\u001B[0;32m   1684\u001B[0m     callbacks\u001B[38;5;241m.\u001B[39mon_train_batch_begin(step)\n\u001B[1;32m-> 1685\u001B[0m     tmp_logs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1686\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m data_handler\u001B[38;5;241m.\u001B[39mshould_sync:\n\u001B[0;32m   1687\u001B[0m         context\u001B[38;5;241m.\u001B[39masync_wait()\n",
      "File \u001B[1;32mC:\\dev\\repos\\FH\\Computer_Vision\\venv\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    148\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    149\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 150\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    151\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    152\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[1;32mC:\\dev\\repos\\FH\\Computer_Vision\\venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:894\u001B[0m, in \u001B[0;36mFunction.__call__\u001B[1;34m(self, *args, **kwds)\u001B[0m\n\u001B[0;32m    891\u001B[0m compiler \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mxla\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jit_compile \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnonXla\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    893\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m OptionalXlaContext(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jit_compile):\n\u001B[1;32m--> 894\u001B[0m   result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    896\u001B[0m new_tracing_count \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexperimental_get_tracing_count()\n\u001B[0;32m    897\u001B[0m without_tracing \u001B[38;5;241m=\u001B[39m (tracing_count \u001B[38;5;241m==\u001B[39m new_tracing_count)\n",
      "File \u001B[1;32mC:\\dev\\repos\\FH\\Computer_Vision\\venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:926\u001B[0m, in \u001B[0;36mFunction._call\u001B[1;34m(self, *args, **kwds)\u001B[0m\n\u001B[0;32m    923\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock\u001B[38;5;241m.\u001B[39mrelease()\n\u001B[0;32m    924\u001B[0m   \u001B[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001B[39;00m\n\u001B[0;32m    925\u001B[0m   \u001B[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001B[39;00m\n\u001B[1;32m--> 926\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_no_variable_creation_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# pylint: disable=not-callable\u001B[39;00m\n\u001B[0;32m    927\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_variable_creation_fn \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    928\u001B[0m   \u001B[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001B[39;00m\n\u001B[0;32m    929\u001B[0m   \u001B[38;5;66;03m# in parallel.\u001B[39;00m\n\u001B[0;32m    930\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock\u001B[38;5;241m.\u001B[39mrelease()\n",
      "File \u001B[1;32mC:\\dev\\repos\\FH\\Computer_Vision\\venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:143\u001B[0m, in \u001B[0;36mTracingCompiler.__call__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    140\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock:\n\u001B[0;32m    141\u001B[0m   (concrete_function,\n\u001B[0;32m    142\u001B[0m    filtered_flat_args) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_maybe_define_function(args, kwargs)\n\u001B[1;32m--> 143\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mconcrete_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_flat\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    144\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfiltered_flat_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcaptured_inputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconcrete_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcaptured_inputs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\dev\\repos\\FH\\Computer_Vision\\venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1757\u001B[0m, in \u001B[0;36mConcreteFunction._call_flat\u001B[1;34m(self, args, captured_inputs, cancellation_manager)\u001B[0m\n\u001B[0;32m   1753\u001B[0m possible_gradient_type \u001B[38;5;241m=\u001B[39m gradients_util\u001B[38;5;241m.\u001B[39mPossibleTapeGradientTypes(args)\n\u001B[0;32m   1754\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (possible_gradient_type \u001B[38;5;241m==\u001B[39m gradients_util\u001B[38;5;241m.\u001B[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001B[0;32m   1755\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m executing_eagerly):\n\u001B[0;32m   1756\u001B[0m   \u001B[38;5;66;03m# No tape is watching; skip to running the function.\u001B[39;00m\n\u001B[1;32m-> 1757\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_call_outputs(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_inference_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcall\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1758\u001B[0m \u001B[43m      \u001B[49m\u001B[43mctx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcancellation_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcancellation_manager\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m   1759\u001B[0m forward_backward \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_select_forward_and_backward_functions(\n\u001B[0;32m   1760\u001B[0m     args,\n\u001B[0;32m   1761\u001B[0m     possible_gradient_type,\n\u001B[0;32m   1762\u001B[0m     executing_eagerly)\n\u001B[0;32m   1763\u001B[0m forward_function, args_with_tangents \u001B[38;5;241m=\u001B[39m forward_backward\u001B[38;5;241m.\u001B[39mforward()\n",
      "File \u001B[1;32mC:\\dev\\repos\\FH\\Computer_Vision\\venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:381\u001B[0m, in \u001B[0;36m_EagerDefinedFunction.call\u001B[1;34m(self, ctx, args, cancellation_manager)\u001B[0m\n\u001B[0;32m    379\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m _InterpolateFunctionError(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    380\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m cancellation_manager \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 381\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[43mexecute\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexecute\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    382\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msignature\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    383\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnum_outputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_num_outputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    384\u001B[0m \u001B[43m        \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    385\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattrs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattrs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    386\u001B[0m \u001B[43m        \u001B[49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mctx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    387\u001B[0m   \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    388\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m execute\u001B[38;5;241m.\u001B[39mexecute_with_cancellation(\n\u001B[0;32m    389\u001B[0m         \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msignature\u001B[38;5;241m.\u001B[39mname),\n\u001B[0;32m    390\u001B[0m         num_outputs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_outputs,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    393\u001B[0m         ctx\u001B[38;5;241m=\u001B[39mctx,\n\u001B[0;32m    394\u001B[0m         cancellation_manager\u001B[38;5;241m=\u001B[39mcancellation_manager)\n",
      "File \u001B[1;32mC:\\dev\\repos\\FH\\Computer_Vision\\venv\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:52\u001B[0m, in \u001B[0;36mquick_execute\u001B[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[0m\n\u001B[0;32m     50\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     51\u001B[0m   ctx\u001B[38;5;241m.\u001B[39mensure_initialized()\n\u001B[1;32m---> 52\u001B[0m   tensors \u001B[38;5;241m=\u001B[39m \u001B[43mpywrap_tfe\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTFE_Py_Execute\u001B[49m\u001B[43m(\u001B[49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_handle\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mop_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     53\u001B[0m \u001B[43m                                      \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattrs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_outputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     54\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m core\u001B[38;5;241m.\u001B[39m_NotOkStatusException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m     55\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Load a VGG19 model with random initialized weights\n",
    "pre_trained_model = VGG19(weights='imagenet', include_top=False, input_shape=(img_height, img_width, 3))\n",
    "\n",
    "# Create a new model\n",
    "trained_model = Sequential()\n",
    "# Add the base model as the first layer\n",
    "trained_model.add(pre_trained_model)\n",
    "# Flatten the output of the base model\n",
    "trained_model.add(Flatten())\n",
    "# Add the final output layer with softmax activation\n",
    "trained_model.add(Dense(len(classes), activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "trained_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "trained_history  = trained_model.fit(\n",
    "    train_generator,\n",
    "    validation_data=validation_generator,\n",
    "    epochs=10)\n",
    "\n",
    "trained_model.save(\"models/trained_model_raw_data.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The differences in loss and accuracy of the plain and pre trained network over the first 10 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract loss and accuracy values for both models\n",
    "plain_loss = plain_history.history['loss']\n",
    "plain_accuracy = plain_history.history['accuracy']\n",
    "trained_loss = trained_history.history['loss']\n",
    "trained_accuracy = trained_history.history['accuracy']\n",
    "\n",
    "# Plot the loss curves\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(1, 11), plain_loss, 'b-', label='Plain Model')\n",
    "plt.plot(range(1, 11), trained_loss, 'r-', label='Pre-trained Model')\n",
    "plt.title('Loss Curves')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot the accuracy curves\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(1, 11), plain_accuracy, 'b-', label='Plain Model')\n",
    "plt.plot(range(1, 11), trained_accuracy, 'r-', label='Pre-trained Model')\n",
    "plt.title('Accuracy Curves')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Data cleansing: Remove “bad” images from the dataset. Which did you remove? How many? Discuss results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Add data augmentation: \n",
    "    - Random flip\n",
    "    - Random contrast\n",
    "    - Random translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_preprocessing(image):\n",
    "    # Generate a random contrast factor\n",
    "    contrast_factor = np.random.uniform(0.8, 1.2)\n",
    "    # Apply contrast adjustment\n",
    "    image = image * contrast_factor\n",
    "    image = tf.clip_by_value(image, 0.0, 1.0)  # Clip values to the [0, 1] range\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters for the loader\n",
    "batch_size = 10\n",
    "img_height = 224\n",
    "img_width = 224\n",
    "\n",
    "# Load the training data\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1. / 255,\n",
    "        validation_split=0.3,\n",
    "        horizontal_flip=True,  # Apply random flip\n",
    "        vertical_flip=True,  # Apply random flip\n",
    "        width_shift_range=0.2,  # Apply random translation\n",
    "        height_shift_range=0.2,  # Apply random translation\n",
    "        preprocessing_function=custom_preprocessing # Apply random contrast\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    base_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='training')  # set as training data\n",
    "\n",
    "# Load the validation data\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    base_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='validation')  # set as validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train both models again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a VGG19 model with random initialized weights\n",
    "base_model = VGG19(weights=None, include_top=False, input_shape=(img_height, img_width, 3))\n",
    "\n",
    "# Create a new model\n",
    "plain_model = Sequential()\n",
    "# Add the base model as the first layer\n",
    "plain_model.add(base_model)\n",
    "# Flatten the output of the base model\n",
    "plain_model.add(Flatten())\n",
    "# Add the final output layer with softmax activation\n",
    "plain_model.add(Dense(len(classes), activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "plain_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "plain_history = plain_model.fit(\n",
    "    train_generator,\n",
    "    validation_data=validation_generator,\n",
    "    epochs=10)\n",
    "\n",
    "plain_model.save(\"plain_model_augmentated_data.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a VGG19 model with random initialized weights\n",
    "pre_trained_model = VGG19(weights='imagenet', include_top=False, input_shape=(img_height, img_width, 3))\n",
    "\n",
    "# Create a new model\n",
    "trained_model = Sequential()\n",
    "# Add the base model as the first layer\n",
    "trained_model.add(pre_trained_model)\n",
    "# Flatten the output of the base model\n",
    "trained_model.add(Flatten())\n",
    "# Add the final output layer with softmax activation\n",
    "trained_model.add(Dense(len(classes), activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "trained_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "trained_history  = trained_model.fit(\n",
    "    train_generator,\n",
    "    validation_data=validation_generator,\n",
    "    epochs=10)\n",
    "\n",
    "trained_model.save(\"trained_model_augmentated_data.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract loss and accuracy values for both models\n",
    "plain_loss = plain_history.history['loss']\n",
    "plain_accuracy = plain_history.history['accuracy']\n",
    "trained_loss = trained_history.history['loss']\n",
    "trained_accuracy = trained_history.history['accuracy']\n",
    "\n",
    "# Plot the loss curves\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(1, 11), plain_loss, 'b-', label='Plain Model')\n",
    "plt.plot(range(1, 11), trained_loss, 'r-', label='Pre-trained Model')\n",
    "plt.title('Loss Curves')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot the accuracy curves\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(1, 11), plain_accuracy, 'b-', label='Plain Model')\n",
    "plt.plot(range(1, 11), trained_accuracy, 'r-', label='Pre-trained Model')\n",
    "plt.title('Accuracy Curves')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Rebuild VGG19. After layer block4_conv4 (25, 25, 512):\n",
    "    - Add inception layer with dimensionality reduction (no of output filters should be 512, choose own values for the filter dimensionality reduction in 1x1 layers)\n",
    "    - Add conv layer (kernel 1x1,  filters 1024, padding valid, stride 1, activation leaky relu)\n",
    "    - Add conv layer (kernel 3x3,  filters 1024, padding same, stride 1, activation relu)\n",
    "    - Freeze conv2 layers and before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a VGG19 model with random initialized weights\n",
    "base_model = VGG19(weights=None, include_top=False, input_shape=(img_height, img_width, 3))\n",
    "\n",
    "# Create a new model\n",
    "plain_model = Sequential()\n",
    "\n",
    "# Add layers until block4_conv4\n",
    "for layer in base_model.layers:\n",
    "    plain_model.add(layer)\n",
    "    if layer.name == 'block4_conv4':\n",
    "        break\n",
    "\n",
    "# Freeze the layers before block2_conv2\n",
    "freeze = True\n",
    "for layer in plain_model.layers:\n",
    "    if layer.name == 'block2_conv2':\n",
    "        freeze = False\n",
    "    layer.trainable = not freeze\n",
    "\n",
    "# Add the inception layer with dimensionality reduction\n",
    "plain_model.add(Conv2D(512, (1, 1), activation='relu'))\n",
    "plain_model.add(Conv2D(256, (1, 1), activation='relu'))\n",
    "plain_model.add(Conv2D(512, (3, 3), padding='same', activation='relu'))\n",
    "\n",
    "# Add the conv layer with kernel 1x1, filters 1024, padding valid, stride 1, activation leaky relu\n",
    "plain_model.add(Conv2D(1024, (1, 1), padding='valid', strides=1, activation=LeakyReLU(alpha=0.1)))\n",
    "\n",
    "# Add the conv layer with kernel 3x3, filters 1024, padding same, stride 1, activation relu\n",
    "plain_model.add(Conv2D(1024, (3, 3), padding='same', strides=1, activation='relu'))\n",
    "\n",
    "# Flatten the output of the last added layer\n",
    "plain_model.add(Flatten())\n",
    "\n",
    "# Add the final output layer with softmax activation\n",
    "plain_model.add(Dense(len(classes), activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "plain_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "plain_history = plain_model.fit(\n",
    "    train_generator,\n",
    "    validation_data=validation_generator,\n",
    "    epochs=10)\n",
    "\n",
    "plain_model.save(\"plain_Rebuild_VGG19_model_augmentated_data.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a VGG19 model with random initialized weights\n",
    "pre_trained_model = VGG19(weights='imagenet', include_top=False, input_shape=(img_height, img_width, 3))\n",
    "\n",
    "# Create a new model\n",
    "trained_model = Sequential()\n",
    "\n",
    "# Add layers until block4_conv4\n",
    "for layer in pre_trained_model.layers:\n",
    "    trained_model.add(layer)\n",
    "    if layer.name == 'block4_conv4':\n",
    "        break\n",
    "\n",
    "# Freeze the layers before block2_conv2\n",
    "freeze = True\n",
    "for layer in trained_model.layers:\n",
    "    if layer.name == 'block2_conv2':\n",
    "        freeze = False\n",
    "    layer.trainable = not freeze\n",
    "\n",
    "# Add the inception layer with dimensionality reduction\n",
    "trained_model.add(Conv2D(512, (1, 1), activation='relu'))\n",
    "trained_model.add(Conv2D(256, (1, 1), activation='relu'))\n",
    "trained_model.add(Conv2D(512, (3, 3), padding='same', activation='relu'))\n",
    "\n",
    "# Add the conv layer with kernel 1x1, filters 1024, padding valid, stride 1, activation leaky relu\n",
    "trained_model.add(Conv2D(1024, (1, 1), padding='valid', strides=1, activation=LeakyReLU(alpha=0.1)))\n",
    "\n",
    "# Add the conv layer with kernel 3x3, filters 1024, padding same, stride 1, activation relu\n",
    "trained_model.add(Conv2D(1024, (3, 3), padding='same', strides=1, activation='relu'))\n",
    "\n",
    "# Flatten the output of the last added layer\n",
    "trained_model.add(Flatten())\n",
    "\n",
    "# Add the final output layer with softmax activation\n",
    "trained_model.add(Dense(len(classes), activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "trained_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "trained_history  = trained_model.fit(\n",
    "    train_generator,\n",
    "    validation_data=validation_generator,\n",
    "    epochs=10)\n",
    "\n",
    "trained_model.save(\"trained_Rebuild_VGG19_model_augmentated_data.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract loss and accuracy values for both models\n",
    "plain_loss = plain_history.history['loss']\n",
    "plain_accuracy = plain_history.history['accuracy']\n",
    "trained_loss = trained_history.history['loss']\n",
    "trained_accuracy = trained_history.history['accuracy']\n",
    "\n",
    "# Plot the loss curves\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(1, 11), plain_loss, 'b-', label='Plain Model')\n",
    "plt.plot(range(1, 11), trained_loss, 'r-', label='Pre-trained Model')\n",
    "plt.title('Loss Curves')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot the accuracy curves\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(1, 11), plain_accuracy, 'b-', label='Plain Model')\n",
    "plt.plot(range(1, 11), trained_accuracy, 'r-', label='Pre-trained Model')\n",
    "plt.title('Accuracy Curves')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Test a few of your own images and present the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Answer the following questions:\n",
    "    - What accuracy can be achieved? What is the accuracy of the train vs. test set?\n",
    "    - On what infrastructure did you train it? What is the inference time?\n",
    "    - What are the number of parameters of the model?\n",
    "    - Which categories are most likely to be confused by the algorithm? Show results in a confusion matrix."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
