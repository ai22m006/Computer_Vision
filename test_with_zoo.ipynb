{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification Project 6\n",
    "Choose three classes from the Open Images Dataset. Train a neural net that is able to classify images into these three categories.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-08T21:10:47.069806800Z",
     "start_time": "2023-06-08T21:10:47.065808200Z"
    }
   },
   "outputs": [],
   "source": [
    "classes = [\"Cat\", \"Dog\", \"Person\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "https://storage.googleapis.com/openimages/web/visualizer/index.html?type=detection\n",
    "\n",
    "## Base model\n",
    "VGG 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-08T21:10:48.106849300Z",
     "start_time": "2023-06-08T21:10:48.088839500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['activitynet-100', 'activitynet-200', 'bdd100k', 'caltech101', 'caltech256', 'cifar10', 'cifar100', 'cityscapes', 'coco-2014', 'coco-2017', 'fashion-mnist', 'fiw', 'hmdb51', 'imagenet-2012', 'imagenet-sample', 'kinetics-400', 'kinetics-600', 'kinetics-700', 'kinetics-700-2020', 'kitti', 'kitti-multiview', 'lfw', 'mnist', 'open-images-v6', 'open-images-v7', 'quickstart', 'quickstart-geo', 'quickstart-groups', 'quickstart-video', 'sama-coco', 'ucf101', 'voc-2007', 'voc-2012']\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import tensorflow as tf\n",
    "from keras.applications import VGG19\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import fiftyone as fo\n",
    "import fiftyone.zoo as foz\n",
    "\n",
    "print(foz.list_zoo_datasets())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-08T21:11:31.045163500Z",
     "start_time": "2023-06-08T21:10:48.720824700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading split 'train' to 'C:\\Users\\Michael\\fiftyone\\open-images-v7\\train' if necessary\n",
      "Necessary images already downloaded\n",
      "Existing download of split 'train' is sufficient\n",
      "Downloading split 'test' to 'C:\\Users\\Michael\\fiftyone\\open-images-v7\\test' if necessary\n",
      "Necessary images already downloaded\n",
      "Existing download of split 'test' is sufficient\n",
      "Downloading split 'validation' to 'C:\\Users\\Michael\\fiftyone\\open-images-v7\\validation' if necessary\n",
      "Necessary images already downloaded\n",
      "Existing download of split 'validation' is sufficient\n",
      "Loading existing dataset 'cdp-dataset'. To reload from disk, either delete the existing dataset or provide a custom `dataset_name` to use\n"
     ]
    }
   ],
   "source": [
    "# Path to the directory where the images are stored\n",
    "base_dir = './dataset'\n",
    "n_images = 100  # number of images per class\n",
    "\n",
    "dataset = foz.load_zoo_dataset(\n",
    "    \"open-images-v7\",\n",
    "    max_samples=n_images,\n",
    "    seed=51,\n",
    "    shuffle=True,\n",
    "    label_types=[\"classifications\"],\n",
    "    classes= classes,\n",
    "    dataset_name=\"cdp-dataset\",\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['hierarchy', 'classes_map'])\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.lib.display.IFrame at 0x17a767fdf10>",
      "text/html": "\n        <iframe\n            width=\"100%\"\n            height=\"800\"\n            src=\"http://localhost:5151/?notebook=True&subscription=62aa9bec-b6c4-4cd5-bd48-1e7420ca4074\"\n            frameborder=\"0\"\n            allowfullscreen\n            \n        ></iframe>\n        "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(dataset.info.keys())\n",
    "\n",
    "session = fo.launch_app(dataset.view())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-08T21:11:35.218813Z",
     "start_time": "2023-06-08T21:11:31.049662300Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task\n",
    "1. Preparation: Split dataset into a 70/30 Train/test split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-08T21:11:45.242579400Z",
     "start_time": "2023-06-08T21:11:45.220554300Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "flow_from_directory() missing 1 required positional argument: 'directory'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[16], line 13\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;66;03m# Load the training data\u001B[39;00m\n\u001B[0;32m      7\u001B[0m train_datagen \u001B[38;5;241m=\u001B[39m ImageDataGenerator(rescale\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1.\u001B[39m \u001B[38;5;241m/\u001B[39m \u001B[38;5;241m255\u001B[39m,\n\u001B[0;32m      8\u001B[0m                                    shear_range\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.2\u001B[39m,\n\u001B[0;32m      9\u001B[0m                                    zoom_range\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.2\u001B[39m,\n\u001B[0;32m     10\u001B[0m                                    horizontal_flip\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m     11\u001B[0m                                    validation_split\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.3\u001B[39m)  \u001B[38;5;66;03m# set validation split\u001B[39;00m\n\u001B[1;32m---> 13\u001B[0m train_generator \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_datagen\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mflow_from_directory\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     14\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtarget_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mimg_height\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mimg_width\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     15\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     16\u001B[0m \u001B[43m    \u001B[49m\u001B[43mclass_mode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mcategorical\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     17\u001B[0m \u001B[43m    \u001B[49m\u001B[43msubset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtraining\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# set as training data\u001B[39;00m\n\u001B[0;32m     19\u001B[0m \u001B[38;5;66;03m# Load the validation data\u001B[39;00m\n\u001B[0;32m     20\u001B[0m validation_generator \u001B[38;5;241m=\u001B[39m train_datagen\u001B[38;5;241m.\u001B[39mflow_from_directory(\n\u001B[0;32m     21\u001B[0m     target_size\u001B[38;5;241m=\u001B[39m(img_height, img_width),\n\u001B[0;32m     22\u001B[0m     batch_size\u001B[38;5;241m=\u001B[39mbatch_size,\n\u001B[0;32m     23\u001B[0m     class_mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcategorical\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m     24\u001B[0m     subset\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mvalidation\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[1;31mTypeError\u001B[0m: flow_from_directory() missing 1 required positional argument: 'directory'"
     ]
    }
   ],
   "source": [
    "# Define parameters for the loader\n",
    "batch_size = 20\n",
    "img_height = 224\n",
    "img_width = 224\n",
    "\n",
    "# Load the training data\n",
    "train_datagen = ImageDataGenerator(rescale=1. / 255,\n",
    "                                   shear_range=0.2,\n",
    "                                   zoom_range=0.2,\n",
    "                                   horizontal_flip=True,\n",
    "                                   validation_split=0.3)  # set validation split\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='training')  # set as training data\n",
    "\n",
    "# Load the validation data\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='validation')  # set as validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Train a VGG19 network from scratch (randomly initialized weights) and estimate the testset accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-08T18:48:09.292182200Z",
     "start_time": "2023-06-08T18:47:59.533523400Z"
    }
   },
   "outputs": [],
   "source": [
    "# load a vgg19 with random init weights\n",
    "random_base_vgg19 = VGG19(weights=None, include_top=False, input_shape=(img_height, img_width, 3))\n",
    "\n",
    "# TODO estimate testset accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Transfer learning: Use an imagenet pretrained VGG19 network, train the model and estimate the testset accuracy. Show the differences in loss and accuracy of the plain and pre trained network over the first 10 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-08T20:00:54.756317900Z",
     "start_time": "2023-06-08T18:48:14.347358300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "157/157 [==============================] - 447s 3s/step - loss: 1.8531 - accuracy: 0.5722 - val_loss: 0.8886 - val_accuracy: 0.6269\n",
      "Epoch 2/10\n",
      "157/157 [==============================] - 482s 3s/step - loss: 0.7020 - accuracy: 0.7166 - val_loss: 0.7868 - val_accuracy: 0.6649\n",
      "Epoch 3/10\n",
      "157/157 [==============================] - 390s 2s/step - loss: 0.6338 - accuracy: 0.7422 - val_loss: 0.7973 - val_accuracy: 0.6552\n",
      "Epoch 4/10\n",
      "157/157 [==============================] - 472s 3s/step - loss: 0.5546 - accuracy: 0.7604 - val_loss: 0.8229 - val_accuracy: 0.6709\n",
      "Epoch 5/10\n",
      "157/157 [==============================] - 436s 3s/step - loss: 0.5016 - accuracy: 0.7885 - val_loss: 0.7594 - val_accuracy: 0.6896\n",
      "Epoch 6/10\n",
      "157/157 [==============================] - 449s 3s/step - loss: 0.4661 - accuracy: 0.8038 - val_loss: 0.7592 - val_accuracy: 0.6881\n",
      "Epoch 7/10\n",
      "157/157 [==============================] - 485s 3s/step - loss: 0.4230 - accuracy: 0.8329 - val_loss: 0.8835 - val_accuracy: 0.6604\n",
      "Epoch 8/10\n",
      "157/157 [==============================] - 390s 2s/step - loss: 0.4483 - accuracy: 0.8137 - val_loss: 0.7909 - val_accuracy: 0.6694\n",
      "Epoch 9/10\n",
      "157/157 [==============================] - 450s 3s/step - loss: 0.3746 - accuracy: 0.8479 - val_loss: 0.8798 - val_accuracy: 0.6791\n",
      "Epoch 10/10\n",
      "157/157 [==============================] - 357s 2s/step - loss: 0.3708 - accuracy: 0.8409 - val_loss: 0.8439 - val_accuracy: 0.6739\n"
     ]
    }
   ],
   "source": [
    "# Load the VGG19 model\n",
    "base_model = VGG19(weights='imagenet', include_top=False, input_shape=(img_height, img_width, 3))\n",
    "\n",
    "# Freeze the layers of the base model\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Create a new model on top of the base model\n",
    "x = Flatten()(base_model.output)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "predictions = Dense(len(classes), activation='softmax')(x)\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // batch_size,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.samples // batch_size,\n",
    "    epochs=10)\n",
    "\n",
    "# Save the model\n",
    "model.save('models/model.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Data cleansing: Remove “bad” images from the dataset. Which did you remove? How many? Discuss results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Add data augmentation and train again, discuss results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Rebuild VGG19. After layer block4_conv4 (25, 25, 512):\n",
    "    - Random flip\n",
    "    - Random contrast\n",
    "    - Random translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Test a few of your own images and present the results\n",
    "    - Add inception layer with dimensionality reduction (no of output filters should be 512, choose own values for the filter dimensionality reduction in 1x1 layers)\n",
    "    - Add conv layer (kernel 1x1,  filters 1024, padding valid, stride 1, activation leaky relu)\n",
    "    - Add conv layer (kernel 3x3,  filters 1024, padding same, stride 1, activation relu)\n",
    "    - Freeze conv2 layers and before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Answer the following questions:\n",
    "    - What accuracy can be achieved? What is the accuracy of the train vs. test set?\n",
    "    - On what infrastructure did you train it? What is the inference time?\n",
    "    - What are the number of parameters of the model?\n",
    "    - Which categories are most likely to be confused by the algorithm? Show results in a confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the results of the experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
